**姓名** 仇鑫

**日期** 2019/3/4 - 2019/3/10

---

### 本周已完成任务

1. 阅读  Geoffrey Hinton, Oriol Vinyals, Jeff Dean << Distilling the Knowledge in a Neural Network >>
2. 搜集可解释性相关论文
3. 搜集整理科技部项目数据分析流程
4. 阅读《深度学习》第三章
5. 阅读《Jointly Embedding Knowledge Graphs and Logical Rules》
6. 阅读《Graph Distillation for Action Detection with Privileged Modalities》
7. 阅读logicnn源码寻找切入点

### 下周计划

1. 思考领域知识与知识蒸馏相结合
2. 阅读 《Revisiting the Importance of Encoding Logic Rules
   in Sentiment Classification》
3. 阅读《Distilling Task Knowledge from How-To Communities》
4. 阅读《A 2-phase Frame-based Knowledge Extraction Framework》
5. 阅读《Deep Neural Networks with Massive Learned Knowledge》

### 总结

《Jointly Embedding Knowledge Graphs and Logical Rules》提出了一种联合嵌入知识图和逻辑规则的新方法。关键思想是在统一框架中表示和建模三元组和规则。三元组表示为原子公式并由翻译假设建模，而规则表示为复杂公式并由t-范数模糊逻辑建模。嵌入相当于最小化原子和复杂公式的全局损失。通过这种方式，学习嵌入不仅与三元组兼容，而且与规则兼容，这对于知识获取和推理肯定更具预测性。本文使用链接预测和三重分类任务来评估这种方法，它增强了对新事实的预测，这些事实甚至无法通过纯逻辑推理直接推断。

《Graph Distillation for Action Detection with Privileged Modalities》提出了一种技术，可以在现实和具有挑战性的条件下处理多视频中的动作检测，其中只有有限的训练数据和部分观察到的模态。 迁移学习中的常用方法不利用源域中可能提供的额外模态。 另一方面，以前关于多模式学习的工作仅关注单个领域或任务，并不处理训练和测试之间的模态差异。 在这项工作中，我们提出了一种称为图形蒸馏的方法，该方法结合了源域中大规模多模态数据集的丰富特权信息，并改善了训练数据和模态稀缺的目标领域的学习。



KP散度是用来衡量对于同一个随机变量x的两个单独的概率分布P(x)和Q(x)的差异，KL散度是不对称的，且值是大于等于0的。当且仅当两分布相同时，KL散度等于0。