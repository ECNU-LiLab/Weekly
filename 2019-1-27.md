**姓名** 仇鑫

**日期** 2019/1/21 - 2019/1/27

---

### 本周已完成任务

1. 阅读《Hands On Machine Learning with Scikit Learn and TensorFlow》自动编码器部分
2. 阅读论文[The Mythos of Model Interpretability](https://arxiv.org/pdf/1606.03490.pdf)
3. 阅读论文[Interpretability via Model Extraction](https://arxiv.org/pdf/1706.09773.pdf)

### 下周计划

1. 阅读 << Word2Vec Tutorial - The Skip-Gram Model >>
2. 阅读 << [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)>>
3. 阅读<<[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)>>

### 总结

The Mythos of Model Interpretability 讨论了监督式机器学习预测模型的可解释性问题，首先回顾了他人的关于可解释性的论文，大家对可解释性的定义还不明确，甚至还存在不一致。接着将可解释性定义为对人透明且存在因果解释。作者认为深度神经网络并非不如线性模型的解释性。

当前大部分的论文对可解释性的定义都是模型是否可理解，也就是说是否我们能理解模型是如何工作的，这也就是模型的透明性。另一种是所谓的事后解释作为可解释性，这种解释无关模型的运作，通过结果来解释为什么这么做。可解释性想要的是可信度，因果性，可转移性，有信息性，做出公正有伦理的决策。

未来有几个方向，通过更丰富的损失函数和性能尺度来缓和现实生活与机器学习目标之间的差异，比如 sparsity-inducing regularizers 和 cost-sensitive learning 。另一个方向是将可解释分析应用在强化学习上中，强化学习可以学习因果关系。

Interpretability via Model Extraction 这篇论文是通过构建近似模型去解释复杂的模型。本文提出了模型提取这种技术，找出复杂的模型的一个近似模型T，这里论文的近似模型是决策树，因为决策树可解释性高，利用这个近似模型的各种行为来解释复杂模型的作用。本文的关键是获取精确度高且没有过拟合的贪心决策树模型。