**姓名** 仇鑫

**日期** 2019/4/1 - 2019/4/7

---

### 本周已完成任务

1. 阅读《Harnessing Deep Neural Networks with Logic Rules》和源码，尝试理清教师网络和学生网络互相影响的过程
2. 思考将《Improving the Interpretability of Deep Neural Networks with Knowledge Distillation》中的多分类决策树变成迭代的更新过程
3. 阅读《Python自然语言处理》

### 下周计划

1. 阅读 logicnn，修改代码
2. 阅读Python自然语言处理
3. 将 《Harnessing Deep Neural Networks with Logic Rules》的主要公式整理出来，考虑如何修改公式将学生网络替换成决策树
4. 阅读《[Hierarchical interpretations for neural network predictions](https://openreview.net/pdf?id=SkEqro0ctQ)》

### 总结



**相对熵（relative entropy）**就是KL散度（Kullback–Leibler divergence），用于衡量两个概率分布之间的差异。

对于两个概率分布![p(x)](https://www.zhihu.com/equation?tex=p%28x%29)和![q(x)](https://www.zhihu.com/equation?tex=q%28x%29)，其相对熵的计算公式为：

$ \tt KL\it(p\parallel q)=-\int p(x)\ln q(x) dx -(-\int p(x)\ln p(x) dx)$

注意：由于![p(x)](https://www.zhihu.com/equation?tex=p%28x%29)和![q(x)](https://www.zhihu.com/equation?tex=q%28x%29)在公式中的地位不是相等的，所以![\tt KL \it(p\parallel q)\not\equiv \tt KL \it (q\parallel p)](https://www.zhihu.com/equation?tex=%5Ctt+KL+%5Cit%28p%5Cparallel+q%29%5Cnot%5Cequiv+%5Ctt+KL+%5Cit+%28q%5Cparallel+p%29).

相对熵的特点，是只有![p(x)=q(x)](https://www.zhihu.com/equation?tex=p%28x%29%3Dq%28x%29)时，其值为0。若![p(x)](https://www.zhihu.com/equation?tex=p%28x%29)和![q(x)](https://www.zhihu.com/equation?tex=q%28x%29)略有差异，其值就会大于0。其证明利用了负对数函数（![-\ln x](https://www.zhihu.com/equation?tex=-%5Cln+x)）是严格凸函数（strictly convex function）的性质。具体可以参考*PRML*1.6.1 Relative entropy and mutual information.

相对熵公式的前半部分![-\int p(x)\ln q(x)dx](https://www.zhihu.com/equation?tex=-%5Cint+p%28x%29%5Cln+q%28x%29dx)就是**交叉熵（cross entropy）。**

若![p(x)](https://www.zhihu.com/equation?tex=p%28x%29)是数据的真实概率分布，![q(x)](https://www.zhihu.com/equation?tex=q%28x%29)是由数据计算得到的概率分布。机器学习的目的就是希望![q(x)](https://www.zhihu.com/equation?tex=q%28x%29)尽可能地逼近甚至等于![p(x)](https://www.zhihu.com/equation?tex=p%28x%29)，从而使得相对熵接近最小值0. 由于真实的概率分布是固定的，相对熵公式的后半部分![(-\int p(x)\ln p(x) dx)](https://www.zhihu.com/equation?tex=%28-%5Cint+p%28x%29%5Cln+p%28x%29+dx%29)就成了一个常数。那么相对熵达到最小值的时候，也意味着交叉熵达到了最小值。对![q(x)](https://www.zhihu.com/equation?tex=q%28x%29)的优化就等效于求交叉熵的最小值。另外，对交叉熵求最小值，也等效于求最大似然估计（maximum likelihood estimation）。具体可以参考*Deep Learning* 5.5 Maximum Likelihood Estimation.

[引用：Agenter-知乎-如何通俗的解释交叉熵与相对熵?]https://www.zhihu.com/question/41252833/answer/141598211

所以ACL16在进行模型更新时，是对 negative_log_likelihood 作为cost函数，关于模型结构的设置以及如何更新参数，大部分都是根据[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)这篇论文的源码进行修改的，以下是一些思路记录。实际上教师网络只是在学生网络上添加了逻辑规则的分布，之后通过教师网络的loss和学生网络的loss一起通过SGD对学生网络使用的模型进行反向传播，这一步的作用可以看成是将逻辑的影响到学生网络，这也是知识蒸馏。论文里学生网络投影到教师空间，其实就是将学生预测的结果传到教师网络中，之后添加上逻辑就构成了教师网络模型。

后验正则化可以作为引入领域知识的一种很重要的工具，《[Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization](http://nlp.csai.tsinghua.edu.cn/~ly/papers/acl2017_zjc.pdf)》《[Unsupervised Neural Machine Translation with
SMT as Posterior Regularization](https://arxiv.org/pdf/1901.04112.pdf)》这两篇论文就是利用后验正则化引入知识进行机器翻译。

