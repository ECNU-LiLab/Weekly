**姓名** 仇鑫

**日期** 2019/12/2 - 2019/12/8

------

### 本周已完成任务

1. 阅读《A Survey of Methods for Explaining Black Box Models 》
2. 思考实验方案

### 总结

《A Survey of Methods for Explaining Black Box Models》

论文摘要：近年来，许多精确的决策支持系统被构建为黑盒，即向用户隐藏内部逻辑的系统。这种缺乏解释的情况既是一个现实问题，也是一个伦理问题。文献报告了许多旨在克服这一关键弱点的方法，有时以牺牲可解释性的准确性为代价。可以使用黑盒决策系统的应用程序是多种多样的，每种方法通常都是为了为特定问题提供解决方案而开发的，因此，它显式或隐式地描述了自己对解释和解释的理解。本文的目的是就解释的概念和黑箱系统的类型对文献中涉及的主要问题进行分类。对于一个问题的解答，一个黑箱类型，以及一个需要的解释，这个调查应该帮助研究者发现这些建议对他自己的工作更有用。提出的对打开黑箱模型的方法进行分类的方法也应该有助于正确地看待许多研究的开放问题。

首先介绍了可解释性的背景，之后讨论了什么是可解释性，这里论文提出了三种维度：

1.全局和局部可解释性:模型可以完全可解释性，即我们能够理解一个模型的整个逻辑，并遵循整个推理导致所有不同的可能结果。在这种情况下，我们讨论的是全局可解释性。相反，我们用局部可解释性指出，在这种情况下，只可能理解特定决策的原因:只有单个预测/决策是可解释性的。

2.时间限制:一个重要方面是用户可用的时间或允许用户花在理解解释上的时间。用户时间可用性与必须使用预测模型的场景密切相关。因此，在某些情况下，用户需要快速做出决策(例如，灾难即将来临)，最好有一个简单易懂的解释。

3.用户专业知识的性质:预测模型的用户在任务中可能具有不同的背景知识和经验:决策者、科学家、法规遵循和安全工程师、数据科学家等等。了解任务中的用户体验是模型可解释性感知的一个关键方面。领域专家可能更喜欢更大、更复杂的模型，而不是更小、有时更不透明的模型。

本周在思考新的方案的同时，在回顾之前的代码，尝试修改成多次采样而不是一次采样。