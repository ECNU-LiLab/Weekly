**姓名** 仇鑫

**日期** 2019/2/18 - 2019/2/24

---

### 本周已完成任务

1. 阅读<< Harnessing Deep Neural Networks with Logic Rules >>
2. 阅读 << Unsupervised Learning of Neural Networks to Explain Neural Networks >>
3. 调试 << [Network Dissection: Quantifying Interpretability of Deep Visual Representations](http://netdissect.csail.mit.edu/) >> 论文的源码 [NetDissect-Lite](https://github.com/CSAILVision/NetDissect-Lite)
4. 阅读 << [Network Dissection: Quantifying Interpretability of Deep Visual Representations](http://netdissect.csail.mit.edu/) >>

### 下周计划

1. 阅读  Geoffrey Hinton, Oriol Vinyals, Jeff Dean << Distilling the Knowledge in a Neural Network >>
2. 阅读 Nathanael Romano and Robin Schucker << Distilling Knowledge to Specialist Networks for Clustered Classification >>

### 总结

本周选读 AAAI 2019 Workshop 已公开的关于Network Interpretability for Deep Learning的[相关论文](https://arxiv.org/html/1901.08813)。

《Unsupervised Learning of Neural Networks to Explain Neural Networks》认为使用CNN得到的预测结果可信度不能得到很好的保证，因此提出了一种通用工具用来检测CNN中间层特征来保证应用的安全可靠性。该工具给预训练好的模型增加一个解释器，这个解释器可以看成是一个自动编码器，来解释这个训练好的模型。这样的好处是不牺牲原模型的性能从而做到解释黑盒模型。

解释层的工作是这样的，输入的数据同时进入两条路线，一条路线进入可解释路径，其结构是下面绿色方块构成的网络模型，另一条路线是蓝色方块构建的模型，可以发现可解释路线由可解释Conv层，MASK层以及Loss函数构成。之后将结果通过Decoder层进行汇总。

上面这篇论文与《[Network Dissection: Quantifying Interpretability of Deep Visual Representations](http://netdissect.csail.mit.edu/)》论文的目的类似，都是想画出影响识别主体的特征边界。





