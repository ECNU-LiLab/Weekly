**姓名** 仇鑫

**日期** 2019/10/28 - 2019/11/3

------

### 本周已完成任务

修改知识蒸馏之前的模型

### 总结

本周之前的工作是对之前的工作进行比对

1.使用Anchors Demo里的随机森林模型，测试集上的精度是0.7402526084568918，和我们的精度相似，我们GBDT模型会稍微比他高一点

2.Anchors 使用 GBDT，测试集精度0.6523887973640856，我们使用神经网络再知识蒸馏之后，我们的精度是0.7435475013728721

关于使用logits还是softlabel，这几天进行了细致的研究，得出的结论是，我的模型和其他任务都应该使用Softlabel而不是logits。 PyTorch在使用CrossEntropyLoss时，会自动的将传入的Logits计算出log_softmax，从而进行计算CrossEntropy，所以我这里神经网络的训练也是使用softLabel。但之前工作错误的使用了logits给GBDT，在经过了输出端的修改后，此时重新使用Embedding的值和Softlabel给GBDT进行训练，没有进行仔细调参的情况下，准确度为0.7512355848434926。

