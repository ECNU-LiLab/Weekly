**姓名** 仇鑫

**日期** 2018/12/10 - 2018/12/16

---

### 本周已完成任务

1. 撰写项目文档
2. 阅读 AI2: Safety and Robustness Certification of Neural
   Networks with Abstract Interpretation
3. 阅读《Hands On Machine Learning with Scikit Learn and TensorFlow》
4. 对可解释性进行Literary Survey

### 下周计划

1. 对可解释性进行Literary Survey
2. 阅读《Hands On Machine Learning with Scikit Learn and TensorFlow》

### 总结

关于可解释性，比较早的是ACM SIGKDD 2013上的这篇Comprehensible classification models: a position paper，这篇论文认为，绝大多数论文仅使用预测准确性标准来评估分类模型的性能。 该文文在分类模型上考虑了可理解性（可解释性），并讨论了五种分类模型的可解释性，即决策树，分类规则，决策表，最近邻和贝叶斯网络分类器。 该文讨论了特定于每种模型类型的可解释性问题和更通用的可解释性问题，即使用模型大小作为评估模型可理解性的唯一标准的缺点，以及使用单调性约束来提高模型的可理解性和可接受性。该文有两大目标：（a）讨论五种类型的分类知识表示的优缺点 - 决策树，分类规则，决策表，最近邻和贝叶斯网络分类器，它们的可解释性;(b) 讨论如何提高分类模型的可理解性。

另外通过查阅资料，关于可解释性，大概可以分为三大类：

1. 在建模之前的可解释性方法

2. 建立本身具备可解释性的模型

3. 在建模之后使用可解释性方法对模型作出解释



深度学习中的可解释性，就图像处理层面，Grad-CAM:
Visual Explanations from Deep Networks via Gradient-based Localization 这篇论文是用来解释CNN，是基于 Learning Deep Features for Discriminative Localization 这篇论文的基础上提出的论文，主要是为了解决CNN模型的不可见问题，CAM论文中通过Class Activation Mapping技术将模型中感兴趣的区域用热力图的方式区分出来。作者在这篇文章中提出了一个新的概念，叫做Grad-CAM，全称Gradient-weighted Class Activation Mapping。与CAM不同的是，文章使用了感兴趣、或者说是可指定类的梯度去指明了CNN模型是通过原图中的哪一部分区域得到这个分类结果，并且这个方法可以很方便的扩展到目前的任意一个训练完好的CNN模型中。通过合并Guided Backpropagation的结果，Grad-CAM还可以做到更细粒度的可视化分析，解释了为什么模型将原图分类到某一类的结果。Grad-CAM可以很方便的推广到图像分类、图像描述和视觉问答等任务的可视化分析中。

