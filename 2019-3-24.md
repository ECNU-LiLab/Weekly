**姓名** 仇鑫

**日期** 2019/3/18 - 2019/3/24

---

### 本周已完成任务

1. 准备《Revisiting the Importance of Encoding Logic Rules in Sentiment Classification》的pre
2. 搜集知识蒸馏与可解释性相结合的论文
3. 阅读《Improving the Interpretability of Deep Neural
   Networks with Knowledge Distillation》
4. 阅读 logicnn，准备修改代码
5. 弄清知识蒸馏具体作用

### 下周计划

1. 阅读《Interpreting Blackbox Models via Model Extraction》
2. 阅读《Learning Interpretable Models with Causal Guarantees》
3. 阅读 logicnn，修改代码

### 总结

《Improving the Interpretability of Deep Neural Networks with Knowledge Distillation》讨论了通过知识蒸馏来提升深度神经网络的方法，将复杂网络模型通过知识蒸馏构建出一个具有可解释性的决策树，其中一个主要贡献是解决多输出的回归问题，同时证明看学生网络获得了更好的精度提升。在选择学生网络用哪种决策树时，作者分析了软决策树，GBTs，以及GA2Ms等，最终提出vanilla decision
trees最适合多分类问题。



知识蒸馏

**蒸馏可以提供student在one-shot label上学不到的soft label信息**，这些里面包含了**类别间信息**，以及student小网络学不到而teacher网络可以学到的**特征表示‘知识’**，所以可以提高student网络的精度。

原始模型训练：

1. 根据提出的目标问题，设计一个或多个复杂网络（N1，N2,…,Nt）。

2. 收集足够的训练数据，按照常规CNN模型训练流程，并行的训练1中的多个网络得到。得到（M1,M2,…,Mt）

精简模型训练：

1. 根据（N1，N2,…,Nt）设计一个简单网络N0。

2. 收集简单模型训练数据，此处的训练数据可以是训练原始网络的有标签数据，也可以是额外的无标签数据。

3. 将2中收集到的样本输入原始模型（M1,M2,…,Mt），修改原始模型softmax层中温度参数T为一个较大值如T=20。每一个样本在每个原始模型可以得到其最终的分类概率向量，选取其中概率至最大即为该模型对于当前样本的判定结果。对于t个原始模型就可以t概率向量。然后对t概率向量求取均值作为当前样本最后的概率输出向量，记为soft_target，保存。

4. 标签融合2中收集到的数据定义为hard_target，有标签数据的hard_target取值为其标签值1，无标签数据hard_taret取值为0。Target = a*hard_target + b*soft_target（a+b=1）。Target最终作为训练数据的标签去训练精简模型。参数a，b是用于控制标签融合权重的，推荐经验值为（a=0.1 b=0.9）

5. 设置精简模型softmax层温度参数与原始复杂模型产生Soft-target时所采用的温度，按照常规模型训练精简网络模型。

6. 部署时将精简模型中的softmax温度参数重置为1，即采用最原始的softmax

#####结论

On MNIST

效果非常更好。对于迁移训练集数据中包含无标签数据或者某些类别数据缺失，依然能够有很好的表现。说明该模型具有非常的推广能力。

On Speech Recognition

组合模型中的所有“知识”都可以被蒸馏集成到精简模型中，这样极大的减少部署的难度。

