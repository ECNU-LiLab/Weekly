**姓名** 仇鑫

**日期** 2019/1/28 - 2019/2/3

---

### 本周已完成任务

1. 阅读《Hands On Machine Learning with Scikit Learn and TensorFlow》强化学习并调试相关源码
2. 阅读 << Word2Vec Tutorial - The Skip-Gram Model >>
3. 阅读 << Word2Vec Tutorial Part 2 - Negative Sampling >>
4. 阅读 << [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/1301.3781.pdf)>>
5. 阅读 << [A Retrieve-and-Edit Framework for Predicting Structured Outputs](<https://arxiv.org/pdf/1812.01194.pdf>) >>
6. 调试并运行[logicnn](https://github.com/ZhitingHu/logicnn)
7. 阅读 << Interpretability of Deep Learning Models: A Survey
   of Results >>

### 下周计划

1. 阅读 << Harnessing Deep Neural Networks with Logic Rules >>
2. 阅读<<[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)>>

### 总结

word2vec是用一个一层的神经网络(CBOW的本质)把one-hot形式的词向量映射为分布式形式的词向量，为了加快训练速度，用了Hierarchical softmax，negative sampling 等技巧。

Interpretability of Deep Learning Models: A Survey of Results 这篇论文是关于深度学习模型可解释性的综述，我们所需要的可解释性其实是希望神经网络能够提供对应输出结果的可理解的判断依据，这样就能理解模型是如何工作的。本篇论文通过多个维度概述了模型可解释性，同时分别描述了这几个维度的前沿进展。

可解释性可分为模型透明度和模型功能性。模型透明度就是指是否人可以通过输入数据重现模型的每一步计算得到其结果，模型全部参数是否可以解释以及能否解释学习算法的运作。模型功能性是指提供模型输出结果语义可理解的描述，或者是对模型的可视化，甚至是局部解释，即并不解释整个模型，仅仅解释一组特定输入与输出。